{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of actions possible at every state\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 actions = Left, Right, Top, Bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an observation is a way to describe the current state of the environment\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 4*4 grid with 16 tiles (S, F, H, G)\n",
    "# S = Start\n",
    "# F = Frozen\n",
    "# H = Hole\n",
    "# G = Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the learning rate\n",
    "alpha = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount factor for future rewards\n",
    "gamma = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the key is the current state\n",
    "# current state is when we're in one of the 16 tiles\n",
    "# initialize Q-values of the 4 possible next actions for each state here it is 1\n",
    "q_table = dict([(x, [1, 1, 1, 1]) for x in range(16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1, 1, 1, 1],\n",
       " 1: [1, 1, 1, 1],\n",
       " 2: [1, 1, 1, 1],\n",
       " 3: [1, 1, 1, 1],\n",
       " 4: [1, 1, 1, 1],\n",
       " 5: [1, 1, 1, 1],\n",
       " 6: [1, 1, 1, 1],\n",
       " 7: [1, 1, 1, 1],\n",
       " 8: [1, 1, 1, 1],\n",
       " 9: [1, 1, 1, 1],\n",
       " 10: [1, 1, 1, 1],\n",
       " 11: [1, 1, 1, 1],\n",
       " 12: [1, 1, 1, 1],\n",
       " 13: [1, 1, 1, 1],\n",
       " 14: [1, 1, 1, 1],\n",
       " 15: [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the next action based on the current observation\n",
    "def choose_action(observ):\n",
    "    # the observation will be the one with the maximum Q-value\n",
    "    return np.argmax(q_table[observ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the Q-table using 10000 episodes\n",
    "for i in range(10000):\n",
    "    \n",
    "    observ = env.reset()\n",
    "    \n",
    "    # start exploring the environment with a random action from the current state\n",
    "    action = choose_action(observ)\n",
    "    \n",
    "    prev_observ = None\n",
    "    prev_action = None\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    # run for 2500 time steps in each episode:\n",
    "    for t in range(2500):\n",
    "        # unlock below code to see all episodes\n",
    "        # env.render()\n",
    "        \n",
    "        observ, reward, done, info = env.step(action)\n",
    "        \n",
    "        action = choose_action(observ)\n",
    "        \n",
    "        if not prev_observ is None:\n",
    "            q_old = q_table[prev_observ][prev_action]\n",
    "            q_new = q_old\n",
    "            \n",
    "            if done:\n",
    "                q_new += alpha * (reward - q_old)\n",
    "            else:\n",
    "                q_new += alpha * (reward + gamma * q_table[observ][action] - q_old)\n",
    "            \n",
    "            # update the state table for the previous action with the new values calculated after the current action\n",
    "            new_table = q_table[prev_observ]\n",
    "            new_table[prev_action] = q_new\n",
    "            \n",
    "            q_table[prev_observ] = new_table\n",
    "            \n",
    "        prev_observ = observ\n",
    "        prev_action = action\n",
    "        \n",
    "        if done:\n",
    "            # print(\"Episode {} finished after {} timesteps with r={}.\".format(i, t, reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3885698603675607,\n",
       " 0.9940427822791436,\n",
       " 0.44129983807316486,\n",
       " 0.4350252352588013]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.7614958015917299,\n",
       "  0.23435976860831992,\n",
       "  0.2106409770396447,\n",
       "  0.24294246931801805],\n",
       " 1: [0.1296, 0.16093207705627355, 0.16258607905115027, 0.7708794918206666],\n",
       " 2: [0.2168285711614884,\n",
       "  0.19981899738890785,\n",
       "  0.22855382559673737,\n",
       "  0.7811100112389834],\n",
       " 3: [0.18256698189425305,\n",
       "  0.2216786319497455,\n",
       "  0.21465607206803722,\n",
       "  0.7780041213681337],\n",
       " 4: [0.8073849231816422,\n",
       "  0.21656240491977638,\n",
       "  0.15385676321977987,\n",
       "  0.15883991379284587],\n",
       " 5: [1, 1, 1, 1],\n",
       " 6: [0.007309034812358188,\n",
       "  0.007671710462346887,\n",
       "  0.46198365951212916,\n",
       "  0.0058140609052250115],\n",
       " 7: [1, 1, 1, 1],\n",
       " 8: [0.12951705599999996,\n",
       "  0.158605573750126,\n",
       "  0.20625488437985176,\n",
       "  0.8712593354800227],\n",
       " 9: [0.16342121302317736,\n",
       "  0.9036823542204303,\n",
       "  0.1435495072402577,\n",
       "  0.12067599507915475],\n",
       " 10: [0.657592345520561,\n",
       "  0.06960112303700865,\n",
       "  0.07625077989367843,\n",
       "  0.07462363645343087],\n",
       " 11: [1, 1, 1, 1],\n",
       " 12: [1, 1, 1, 1],\n",
       " 13: [0.3590033347302257,\n",
       "  0.35985599999999995,\n",
       "  0.9372999834898323,\n",
       "  0.26182121952539084],\n",
       " 14: [0.3885698603675607,\n",
       "  0.9940427822791436,\n",
       "  0.44129983807316486,\n",
       "  0.4350252352588013],\n",
       " 15: [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q-values of each action in every state using SARSA\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
